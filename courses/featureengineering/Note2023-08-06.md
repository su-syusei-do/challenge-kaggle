# Mutual Information
相互情報量は相関と非常に似ており、2つの量の間の関係性を測定します。
相互情報量の利点は、どんな種類の関係性も検出できるという点であり、相関は線形関係のみを検出します。

scikit-learnの相互情報量アルゴリズムは、離散的な特徴と連続的な特徴を異なる方法で処理します。
そのため、どの特徴が離散的か連続的かを指定する必要があります。

Scikit-learnのfeature_selectionモジュールには、実数値のターゲットに対する相互情報量メトリック（mutual_info_regression）とカテゴリカルなターゲットに対する相互情報量メトリック（mutual_info_classif）の2つがあります。

```
from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
```

```
def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")


plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)
```
